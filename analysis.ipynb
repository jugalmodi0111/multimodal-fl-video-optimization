{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6312a83",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ce28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ“ Analysis environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ee2ab",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e937d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIGHTWEIGHT DATASET STATISTICS (Memory-Safe)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"              DATASET ANALYSIS REPORT (Lightweight)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. BASIC OVERVIEW\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nðŸ“Š DATASET OVERVIEW\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "total_samples = X.shape[0]\n",
    "n_features = X.shape[1]\n",
    "n_classes = len(np.unique(y))\n",
    "train_samples = X_train.shape[0]\n",
    "test_samples = X_test.shape[0]\n",
    "\n",
    "print(f\"Total Samples:        {total_samples}\")\n",
    "print(f\"Training Samples:     {train_samples} ({train_samples/total_samples*100:.1f}%)\")\n",
    "print(f\"Test Samples:         {test_samples} ({test_samples/total_samples*100:.1f}%)\")\n",
    "print(f\"Feature Dimensions:   {n_features}\")\n",
    "print(f\"Number of Classes:    {n_classes}\")\n",
    "print(f\"Train/Test Ratio:     {train_samples/test_samples:.2f}:1\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. CLASS DISTRIBUTION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nðŸ“ˆ CLASS DISTRIBUTION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "train_counts = np.bincount(y_train)\n",
    "test_counts = np.bincount(y_test)\n",
    "\n",
    "print(\"\\nClass    Train    Test     Total    Train%   Test%\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(n_classes):\n",
    "    train_c = train_counts[i] if i < len(train_counts) else 0\n",
    "    test_c = test_counts[i] if i < len(test_counts) else 0\n",
    "    total_c = train_c + test_c\n",
    "    train_pct = (train_c / train_samples * 100) if train_samples > 0 else 0\n",
    "    test_pct = (test_c / test_samples * 100) if test_samples > 0 else 0\n",
    "    print(f\"A{i:<7} {train_c:<8} {test_c:<8} {total_c:<8} {train_pct:>6.1f}%  {test_pct:>6.1f}%\")\n",
    "\n",
    "# Class balance\n",
    "max_class = train_counts.max()\n",
    "min_class = train_counts.min()\n",
    "imbalance = max_class / min_class if min_class > 0 else float('inf')\n",
    "\n",
    "print(f\"\\nClass Balance:\")\n",
    "print(f\"  Max class size: {max_class} samples\")\n",
    "print(f\"  Min class size: {min_class} samples\")\n",
    "print(f\"  Imbalance ratio: {imbalance:.2f}:1\")\n",
    "\n",
    "if imbalance < 1.5:\n",
    "    print(f\"  âœ… Well-balanced classes\")\n",
    "elif imbalance < 3.0:\n",
    "    print(f\"  âš¡ Moderate imbalance\")\n",
    "else:\n",
    "    print(f\"  âš ï¸  Significant imbalance\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                    ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339eb12",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Comprehensive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac98f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE DATASET VISUALIZATIONS (Memory-Optimized)\n",
    "# ============================================================================\n",
    "\n",
    "def create_comprehensive_dataset_visualizations(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations with memory optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    print(\"ðŸŽ¨ Generating comprehensive visualization dashboard...\")\n",
    "    print(\"This may take 30-60 seconds...\\n\")\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    gs = fig.add_gridspec(4, 3, hspace=0.35, wspace=0.35)\n",
    "    \n",
    "    try:\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # 1. Class Distribution Comparison\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        print(\"  Creating plot 1/10: Class Distribution...\")\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        train_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "        test_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "        \n",
    "        x = np.arange(len(train_dist))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax1.bar(x - width/2, train_dist.values, width, label='Training', \n",
    "                        alpha=0.8, color='#3498db', edgecolor='black', linewidth=0.5)\n",
    "        bars2 = ax1.bar(x + width/2, test_dist.values, width, label='Test', \n",
    "                        alpha=0.8, color='#e74c3c', edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        ax1.set_xlabel('Action Class', fontsize=11, fontweight='bold')\n",
    "        ax1.set_ylabel('Number of Samples', fontsize=11, fontweight='bold')\n",
    "        ax1.set_title('Class Distribution: Training vs Test Split', fontsize=12, fontweight='bold')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels([f'A{i}' for i in range(len(train_dist))], rotation=0)\n",
    "        ax1.legend(fontsize=10)\n",
    "        ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # Main title\n",
    "        fig.suptitle('COMPREHENSIVE DATASET ANALYSIS DASHBOARD', \n",
    "                    fontsize=16, fontweight='bold', y=0.995)\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "        \n",
    "        print(\"\\nâœ… All visualizations created successfully!\")\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error during visualization: {str(e)}\")\n",
    "        print(\"Displaying partial results...\")\n",
    "        plt.show()\n",
    "\n",
    "# Execute visualization\n",
    "try:\n",
    "    print(\"\\nðŸŽ¨ Generating comprehensive visualization dashboard...\\n\")\n",
    "    \n",
    "    create_comprehensive_dataset_visualizations(\n",
    "        X_train, y_train, X_test, y_test\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… Visualization dashboard created successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Visualization failed: {str(e)}\")\n",
    "    print(\"Try reducing dataset size or running visualizations individually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caa4b4f",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Federated Partition Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b19142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON-IID FEDERATED DATA PARTITION ANALYSIS\n",
    "#============================================================================\n",
    "\n",
    "def analyze_federated_partitions(clients_X, clients_y, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Detailed analysis of non-IID federated data partitions\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"              FEDERATED DATA PARTITION ANALYSIS (Non-IID)\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    n_clients = len(clients_X)\n",
    "    n_classes = len(np.unique(y_test))\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 1. Per-Client Statistics\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nðŸ“Š SECTION 1: PER-CLIENT STATISTICS\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    client_stats = []\n",
    "    for client_id in range(n_clients):\n",
    "        X_client = clients_X[client_id]\n",
    "        y_client = clients_y[client_id]\n",
    "        \n",
    "        if len(y_client) == 0:\n",
    "            continue\n",
    "            \n",
    "        unique_classes = np.unique(y_client)\n",
    "        class_counts = pd.Series(y_client).value_counts()\n",
    "        \n",
    "        stats = {\n",
    "            'Client ID': f'Client {client_id + 1}',\n",
    "            'Samples': len(y_client),\n",
    "            'Classes': len(unique_classes),\n",
    "            'Class Coverage': f'{len(unique_classes)}/{n_classes}',\n",
    "            'Coverage %': f'{len(unique_classes)/n_classes*100:.1f}%',\n",
    "            'Dominant Class': f'A{class_counts.idxmax()}',\n",
    "            'Max Samples': class_counts.max(),\n",
    "            'Min Samples': class_counts.min(),\n",
    "            'Imbalance': f'{class_counts.max()/class_counts.min():.2f}:1'\n",
    "        }\n",
    "        client_stats.append(stats)\n",
    "    \n",
    "    stats_df = pd.DataFrame(client_stats)\n",
    "    print(\"\\n\" + stats_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nâœ… Federated partition analysis complete!\\n\")\n",
    "    \n",
    "# Run federated partition analysis\n",
    "if 'client_data' in globals():\n",
    "    clients_X = [d['X'] for d in client_data]\n",
    "    clients_y = [d['y'] for d in client_data]\n",
    "    analyze_federated_partitions(\n",
    "        clients_X, clients_y, X_test, y_test\n",
    "    )\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping partition analysis - run main notebook first to create client_data\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
